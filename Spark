1. Accumulator: Spark Accumulators are shared variables which are only “added” through an associative and commutative operation.
	val accum = sc.longAccumulator("SumAccumulator")

2.Broadcast Variable:
	Broadcast variables are read-only variables that will be cached in all the executors instead of shipping every time with the tasks. broadcast variables are used as lookups without any shuffle, as each executor will keep a local copy of it, so no network I/O overhead is involved here.
	val broadcastVar = sc.broadcast(Array(0, 1, 2, 3))

3. Spark Persistence Storage Levels
	MEMORY_ONLY
	MEMORY_AND_DISK
	MEMORY_ONLY_SER
	MEMORY_AND_DISK_SER
	DISK_ONLY
	
4. Writing a file 
	df2.write.mode(SaveMode.Overwrite).csv("/tmp/spark_output/zipcodes")
	
5. spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  
6. Kafka vs RabbitMQ:
		i. Kafka is having pub sub architechture where RabbitMQ is Messaging queue (JMS based)
		ii. As soon as consumer picked up a message then the message got deleted in rabbitMQ, but in kafka it will there in topic of some time. By default it is 7 days.
		iii. Kafka delivers the message in the order it receives but in rabbitMQ delivery is guaranteed but order of message is not.
		iv. Kafka is a distributed system and having more throughput, more scalability. RabbitMQ is not much scalable.
		
7. Creating spark session
	val spark = SparkSession
  .builder
  .appName("StructuredNetworkWordCount")
  .getOrCreate()

8. Reading from kafka 
	val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .option("includeHeaders", "true")
  .load()
  
9. Windowing
	val windowedCounts = words.groupBy(
	window($"timestamp", "10 minutes", "5 minutes"),
	$"word"
	).count()

10. Handling late data/ Watermark
	val windowedCounts = words
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        window($"timestamp", "10 minutes", "5 minutes"),
        $"word")
    .count()
	
11. Spark join without shuffle: HashPartitioner()

12. Solution for data skewness in spark: 
		i. Repartition in to bigger partition
		ii. Salting Technique (Adding salt ie random number to partition key)
		iii. Isolating Salting
		iv. Isolating Map join
		v. Iterative broadcast join 
		
13. User schema in spark:
	val simpleSchema = StructType(Array(
		StructField("firstname",StringType,true),
		StructField("middlename",StringType,true),
		StructField("lastname",StringType,true),
		StructField("id", StringType, true),
		StructField("gender", StringType, true),
		StructField("salary", IntegerType, true)
	))		
	
	val df = spark.createDataFrame(rdd,simpleSchema)
	df.printSchema()
	
14. What is Executor memory and Storage memory?
	Executors divides it memory in to executor memory and storage memory.
	Executor memory is used for computation in shuffles, joins, sorts and aggregations.
	Storage memory is used for caching and dealing internal data across the cluster.
	When no execution memory is used, storage can acquire all the available memory and vice versa.
	
15. Difference between Tez and MR?
	Tez is a DAG (Directed acyclic graph) architecture. MapReduce tasks combine into a single job that is treated as a node in the DAG, enforcing concurrency and serialization. Tez works very similar to Spark (Tez was created by Hortonworks well before Spark). It Executes the plan but not reads data from disk, perform in memory computation.
	MR reads data from disk, run mapper and stores the data on second disk. Performs sorting and shuffling and store data on third mapper. Then run reducers and storage the data on fourth mapper.
	
16. What is out of memory issue in spark?
	Spark can be go out of memory due to of two reasons: Driver out of memory and Executor out of memory.
		a. Driver out of memory: 
			i. collect operation: The most common reason of driver to go out of memory is collect operation. When collect operation is being performed then 	files from all the executors comes to the driver which breach the memory capacity of driver.
			ii. Broadcast join: Suppose we are having few big files and few small files in executors. When we performs bradcast join these small files is being collected at driver and breach it memory capacity. To avoid this we can increase the driver memory or we can set threshold limit for broadcast table.
		b. Executor out of memory:
			i. YARN memory overhead: YARN memory is off heap memory part of executors. Whatever the strings we create as part of our program spark creates hash table of that which is called intern strings. These intern string are stored here. Spark internal objects are stored here. Apart from this, if we code in any other languages apart from Scala wheather it is python or R then their objects are also get stored in YARN memory. YARN memory is 10% of executor memory. To avoid the YARN memory overhead we have to increase the capacity of YARN memory overhead part.
			ii. High Concurrency: Let say we have multiple executors in a machine and we have assigned cores to executor without thinking of the capacity of our machine. Let say we have assigned 10 cores to each executors then it will pick up 10 partitions and process them. Each partition will have their own metadata and overhead memory requirement. Total available executor-memory will be divided to these 10 cores.So each executors have to store huge metadata. Best practice is to assign 4-5 cores to each executors.
			iii. Big Partitions:
		
17. How to choose number of executors and memory?
	Let we are having 6 machines with 16 cores and 64 GB RAM each ie. total 96 cores and 384 GB RAM.
	Smallest executor will be 16/16 core and 64/16 GB of RAM. 96/6 executors on each machine.

18. Repartition vs Coalesce

19. What kind of problems we will face when there are a lot of small files in hadoop?
	i. If we have lots of small files in cluster that will increase burden on namenode. bcoz namenode stores the meta data of file so if we have lots of small files name node keep noting address of files and hence if master down cluster also gone down.
	ii. Spark will also need to create more executor tasks... This will create unnecessary overhead and slow down your data processing.
	
20. Spark yarn cluster vs client?
	In cluster mode, the driver runs in the Application Master, which means that same process is responsible for both driving the application and requesting resources from YARN. In client mode, the driver runs in the client machine.
	In cluster mode, Spark does not support spark-shell. But client mode supports spark-shell.
	
21. Difference between functional and imperative programming?

22. Optimization techniques:
	i. Serialization: By default, Spark uses Java serializer. Spark can also use another serializer called ‘Kryo’ serializer for better performance. Kryo serializer is in compact binary format and offers processing 10x faster than Java serializer. To set the serializer properties:
	conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)
	
23. How to add index column in spark dataframe?
	df.withColumn("id",monotonicallyIncreasingId)
	
24. How to limit the number of retries on Spark job failure in YARN?
	spark-submit --conf spark.yarn.maxAppAttempts=1 <application_name>
	
25. Is there any way to get Spark Application id, while running a job?
	sc.applicationId
	
26. A spark job is getting failed with "no space left on device" but when I ran df -h to check the space then it is showing it is having space. How will you fix this.
		By default Spark uses the /tmp directory to store intermediate data. If you actually do have space left on some device -- you can alter this by creating the file SPARK_HOME/conf/spark-defaults.conf and adding the line. Here SPARK_HOME is wherever you root directory for the spark install is.
		spark.local.dir                     SOME/DIR/WHERE/YOU/HAVE/SPACE

27. How to open .zip files through spark?
		import zipfile
		import io
	
		def zip_extract(x):
		in_memory_data = io.BytesIO(x[1])
		file_obj = zipfile.ZipFile(in_memory_data, "r")
		files = [i for i in file_obj.namelist()]
		return dict(zip(files, [file_obj.open(file).read() for file in files]))


		zips = sc.binaryFiles("hdfs:/Testing/*.zip")
		files_data = zips.map(zip_extract).collect()

28. How to read a AWS S3 file in Spark?
		val spark: SparkSession = SparkSession.builder()
			.master("local[1]")
			.appName("SparkByExamples.com")
			.getOrCreate()
		// Replace Key with your AWS account key (You can find this on IAM 
		spark.sparkContext
			.hadoopConfiguration.set("fs.s3a.access.key", "awsaccesskey value")
		service)
		// Replace Key with your AWS secret key (You can find this on IAM 
		spark.sparkContext
			.hadoopConfiguration.set("fs.s3a.secret.key", "aws secretkey value")
		spark.sparkContext
			.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
		println("##spark read text files from a directory into RDD")
		val rddFromFile = spark.sparkContext.textFile("s3a://sparkbyexamples/csv/text01.txt")
		println(rddFromFile.getClass)
		
		println("##Get data Using collect")
		rddFromFile.collect().foreach(f=>{
			println(f)
		})

30. How to change column types in Spark SQL DataFrame?
		val df2 = df.withColumn("yearTmp", df.year.cast(IntegerType))
			.drop("year")
			.withColumnRenamed("yearTmp", "year")
		
31. I've got big RDD(1gb) in yarn cluster. I can't use collect() How to handle this?
		RDD.toLocalIterator method an efficient way to do the job. It uses runJob to evaluate only a single partition on each step.
		As for the toLocalIterator, it is used to collect the data from the RDD scattered around your cluster into one only node, the one from which the program is running, and do something with all the data in the same node. It is similar to the collect method, but instead of returning a List it will return an Iterator.
		rdd.toLocalIterator()

32. Is there any way for Spark to create primary keys?
		df.withColumn("id", monotonicallyIncreasingId).show()
		monotonicallyIncreasingId function which should work just fine as long as you don't require consecutive numbers.
		
33. How to add a constant column in a Spark DataFrame?
		val newdf = df.withColumn("newcol",lit("myval"))

34. While processing CSV file resultant output is multiple file, wanted single file?

35. How do I skip a header from CSV files in Spark?

36. Explain the difference between Spark SQL and Hive?

37. How to transpose Spark DataFrame?
		val new_schema = StructType(df1.select(collect_list("Column")).first().getAs[Seq[String]](0).map(z => StructField(z, StringType)))
		val new_values = sc.parallelize(Seq(Row.fromSeq(df.select(collect_list("Value")).first().getAs[Seq[String]](0))))
		sqlContext.createDataFrame(new_values, new_schema).show(false)
		
38. What is narrow and wide dependency?
	Narrow Transformation: When each partition at the parent RDD is used by at most one partition of the child RDD or when each partition from child produced or dependent on single parent RDD.
	Eg: map, flatmap, filter
	Wide Transformation: When each partition at the parent RDD is used by multiple partitions of the child RDD or when each partition from child produced or dependent on multiple parent RDD.
	Eg: join, groupbykey
39. Difference between Aggregatebykey & combinebykey?
40. How to disable broadcast?
    spark. sql. autoBroadcastJoinThreshold = -1
41. How to create replication in spark?
42. Draw Spark architecture.
43. How to add one row in dataframe?
44. How to add column in dataframe?
    val df = df1.withColumn("newCol", df1("col") + 1) 
    Or
    val newDf = df.withColumn("D", when($"B".isNull or $"B" === "", 0).otherwise(1))
45. what is exploid function in spark?
46. What is Speculation execution?
47. Create dataframe with JSON file and display some selected fields.
48. How to Search table in hive through Spark?
49. Aggregate function in spark and how its working?
50. How to achive increamental insert and upload in Spark?
51. How many job and Stage will be there in spark?
52. How to assign flag D for duplicates and v for unique based on name field in dataframe?
	val df_count=df.select(*).groupBy("col1").count()
	val df_flag= df_count.withColumn("Flag",when ("count>1","D").otherwise("v"))
53. What is partitioner in spark?
	Partitioner is an object that defines how the elements in a key-value pair RDD are partitioned by key. It maps each key to a partition ID, from 0 to (number of partitions – 1).
	There are 3 types of partitioners in spark: (1) Hash-Partitioner (2) Range-Partitioner (3) One can make its Custom Partitioner
54. Why are we using AVRO fileformat in ingestion of data instead of parquet file?
55. How to check some partitioned are skewed in spark?
    Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:
    A list of scheduler stages and tasks A summary of RDD sizes and memory usage Environmental information.

56. Accumulator is accessible from worker node or not?
    Tasks on worker nodes cannot access the accumulator’s value.Accumulators are write-only variable for worker nodes.
57. How will you handle bad data, let in file few of the values having junk (data of different data type) then should we stop the loading or else can we do?
	a. To include this data in a separate column: As per the use case, if a user wants us to store a bad record in separate column use option mode as “PERMISSIVE”.
	Eg: val data = """{"a": 1, "b":2, "c":3}|{"a": 1, "b":2, "c":3}|{"a": 1, "b, "c":10}""".split('|')
		val corruptDf = spark.read.option("mode", "PERMISSIVE")
                          .option("columnNameOfCorruptRecord", "_corrupt_record")                          
                          .json(sc.parallelize(data)
	b. To ignore all bad records: In this particular use case, if a user doesn’t want to include the bad records at all and wants to store only the correct records use the “DROPMALFORMED” mode.
		val corruptDf = spark.read.option("mode", "DROPMALFORMED")
                          .json(sc.parallelize(data)
	c. Throws an exception when it meets corrupted records: For this use case, if present any bad record will throw an exception. And the mode for this use case will be “FAILFAST”. And it’s a best practice to use this mode in a try-catch block.
		try {
  			val corruptDf = spark.read
  			  .option("mode", "FAILFAST")
  			  .json(sc.parallelize(data))
			} catch {
  			case e:Exception => print(e)
		}

58. What is On-Heap memory and Off-Heap memory?
    Executor acts as a JVM process, and its memory management is based on the JVM. So JVM memory management includes two methods:
    On-Heap memory management: Objects are allocated on the JVM heap and bound by GC.
    Off-Heap memory management: Objects are allocated in memory outside the JVM by serialization, managed by the application, and are not bound by GC. 
        This memory management method can avoid frequent GC, but the disadvantage is that you have to write the logic of memory allocation and memory release.
    In general, the objects' read and write speed is:
    on-heap > off-heap > disk
    
59. Explain typesafety in Dataframe and dataset with example?
	Let we have a case class as:
	case class Student (name : String, Age: Integer, dept: String) 
	and having a rdd named as rdd.
	val dataFrame= rdd.toDF()
	val dataset=rdd.toDS()
	Suppose we need to perform any filter operation:
	val dsResult = dataset.filter(x=>x.age>17)
	This will work fine.
	val dfResult = dataFrame.filter(x=>x.age>17)
	This will through an error, so we can't use column name directly with dataframe, to avoid this we need to typecast.
	val dfResult = dataFrame.filter(x=>x.getAs[Int]["age"]>17)

60. How to select distinct records from a dataframe.
	val dropDisDF = df.dropDuplicates("department","salary")
	
61. Difference between map & map partitioner?
	If there are 2000 row and 20 partitions, then each partition will contain the 2000/20=100 Rows.
	Now, when we apply map(func) method to rdd, the func() operation will be applied on each and every Row and in this particular case func() operation will be called 2000 times. i.e. time-consuming in some time-critical applications.
	If we call mapPartition(func) method on rdd, the func() operation will be called on each partition instead of each row. In this particular case, it will be called 20 times(number of the partition). 
	In this way, you can prevent some processing when it comes to time-consuming application.
