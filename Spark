1. Accumulator: Spark Accumulators are shared variables which are only “added” through an associative and commutative operation.
	val accum = sc.longAccumulator("SumAccumulator")

2.Broadcast Variable:
	Broadcast variables are read-only variables that will be cached in all the executors instead of shipping every time with the tasks. broadcast variables are used as lookups without any shuffle, as each executor will keep a local copy of it, so no network I/O overhead is involved here.
	val broadcastVar = sc.broadcast(Array(0, 1, 2, 3))

3. Spark Persistence Storage Levels
	MEMORY_ONLY
	MEMORY_AND_DISK
	MEMORY_ONLY_SER
	MEMORY_AND_DISK_SER
	DISK_ONLY
	
4. Writing a file 
	df2.write.mode(SaveMode.Overwrite).csv("/tmp/spark_output/zipcodes")
	
5. spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  
6. Kafka vs RabbitMQ:
		i. Kafka is having pub sub architechture where RabbitMQ is Messaging queue (JMS based)
		ii. As soon as consumer picked up a message then the message got deleted in rabbitMQ, but in kafka it will there in topic of some time. By default it is 7 days.
		iii. Kafka delivers the message in the order it receives but in rabbitMQ delivery is guaranteed but order of message is not.
		iv. Kafka is a distributed system and having more throughput, more scalability. RabbitMQ is not much scalable.
		
7. Creating spark session
	val spark = SparkSession
  .builder
  .appName("StructuredNetworkWordCount")
  .getOrCreate()

8. Reading from kafka 
	val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .option("includeHeaders", "true")
  .load()
  
9. Windowing
	val windowedCounts = words.groupBy(
	window($"timestamp", "10 minutes", "5 minutes"),
	$"word"
	).count()

10. Handling late data/ Watermark
	val windowedCounts = words
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        window($"timestamp", "10 minutes", "5 minutes"),
        $"word")
    .count()
	
11. Spark join without shuffle: HashPartitioner()

12. Solution for data skewness in spark: 
		i. Repartition in to bigger partition
		ii. Salting Technique (Adding salt ie random number to partition key)
		iii. Isolating Salting
		iv. Isolating Map join
		v. Iterative broadcast join 
		
13. User schema in spark:
	val simpleSchema = StructType(Array(
		StructField("firstname",StringType,true),
		StructField("middlename",StringType,true),
		StructField("lastname",StringType,true),
		StructField("id", StringType, true),
		StructField("gender", StringType, true),
		StructField("salary", IntegerType, true)
	))		
	
	val df = spark.createDataFrame(rdd,simpleSchema)
	df.printSchema()
	
14. What is Executor memory and Storage memory?
	Executors divides it memory in to executor memory and storage memory.
	Executor memory is used for computation in shuffles, joins, sorts and aggregations.
	Storage memory is used for caching and dealing internal data across the cluster.
	When no execution memory is used, storage can acquire all the available memory and vice versa.
	
15. Difference between Tez and MR?
	Tez is a DAG (Directed acyclic graph) architecture. MapReduce tasks combine into a single job that is treated as a node in the DAG, enforcing concurrency and serialization. Tez works very similar to Spark (Tez was created by Hortonworks well before Spark). It Executes the plan but not reads data from disk, perform in memory computation.
	MR reads data from disk, run mapper and stores the data on second disk. Performs sorting and shuffling and store data on third mapper. Then run reducers and storage the data on fourth mapper.
	
16. What is out of memory issue in spark?
	Spark can be go out of memory due to of two reasons: Driver out of memory and Executor out of memory.
		a. Driver out of memory: 
			i. collect operation: The most common reason of driver to go out of memory is collect operation. When collect operation is being performed then 	files from all the executors comes to the driver which breach the memory capacity of driver.
			ii. Broadcast join: Suppose we are having few big files and few small files in executors. When we performs bradcast join these small files is being collected at driver and breach it memory capacity. To avoid this we can increase the driver memory or we can set threshold limit for broadcast table.
		b. Executor out of memory:
			i. YARN memory overhead: YARN memory is off heap memory part of executors. Whatever the strings we create as part of our program spark creates hash table of that which is called intern strings. These intern string are stored here. Spark internal objects are stored here. Apart from this, if we code in any other languages apart from Scala wheather it is python or R then their objects are also get stored in YARN memory. YARN memory is 10% of executor memory. To avoid the YARN memory overhead we have to increase the capacity of YARN memory overhead part.
			ii. High Concurrency: Let say we have multiple executors in a machine and we have assigned cores to executor without thinking of the capacity of our machine. Let say we have assigned 10 cores to each executors then it will pick up 10 partitions and process them. Each partition will have their own metadata and overhead memory requirement. Total available executor-memory will be divided to these 10 cores.So each executors have to store huge metadata. Best practice is to assign 4-5 cores to each executors.
			iii. Big Partitions:
		
17. How to choose number of executors and memory?
	Let we are having 6 machines with 16 cores and 64 GB RAM each ie. total 96 cores and 384 GB RAM.
	Smallest executor will be 16/16 core and 64/16 GB of RAM. 96/6 executors on each machine.

18. Repartition vs Coalesce

19. What kind of problems we will face when there are a lot of small files in hadoop?
	i. If we have lots of small files in cluster that will increase burden on namenode. bcoz namenode stores the meta data of file so if we have lots of small files name node keep noting address of files and hence if master down cluster also gone down.
	ii. Spark will also need to create more executor tasks... This will create unnecessary overhead and slow down your data processing.
	
20. Spark yarn cluster vs client?
	In cluster mode, the driver runs in the Application Master, which means that same process is responsible for both driving the application and requesting resources from YARN. In client mode, the driver runs in the client machine.
	In cluster mode, Spark does not support spark-shell. But client mode supports spark-shell.
	
21. Difference between functional and imperative programming?

22. Optimization techniques:
	i. Serialization: By default, Spark uses Java serializer. Spark can also use another serializer called ‘Kryo’ serializer for better performance. Kryo serializer is in compact binary format and offers processing 10x faster than Java serializer. To set the serializer properties:
	conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)
	
23. How to add index column in spark dataframe?
	df.withColumn("id",monotonicallyIncreasingId)
	
24. How to limit the number of retries on Spark job failure in YARN?
	spark-submit --conf spark.yarn.maxAppAttempts=1 <application_name>
	
25. Is there any way to get Spark Application id, while running a job?
	sc.applicationId
	
26. A spark job is getting failed with "no space left on device" but when I ran df -h to check the space then it is showing it is having space. How will you fix this.
		By default Spark uses the /tmp directory to store intermediate data. If you actually do have space left on some device -- you can alter this by creating the file SPARK_HOME/conf/spark-defaults.conf and adding the line. Here SPARK_HOME is wherever you root directory for the spark install is.
		spark.local.dir                     SOME/DIR/WHERE/YOU/HAVE/SPACE

27. How to open .zip files through spark?
		import zipfile
		import io
	
		def zip_extract(x):
		in_memory_data = io.BytesIO(x[1])
		file_obj = zipfile.ZipFile(in_memory_data, "r")
		files = [i for i in file_obj.namelist()]
		return dict(zip(files, [file_obj.open(file).read() for file in files]))


		zips = sc.binaryFiles("hdfs:/Testing/*.zip")
		files_data = zips.map(zip_extract).collect()

28. How to read a AWS S3 file in Spark?
		val spark: SparkSession = SparkSession.builder()
			.master("local[1]")
			.appName("SparkByExamples.com")
			.getOrCreate()
		// Replace Key with your AWS account key (You can find this on IAM 
		spark.sparkContext
			.hadoopConfiguration.set("fs.s3a.access.key", "awsaccesskey value")
		service)
		// Replace Key with your AWS secret key (You can find this on IAM 
		spark.sparkContext
			.hadoopConfiguration.set("fs.s3a.secret.key", "aws secretkey value")
		spark.sparkContext
			.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
		println("##spark read text files from a directory into RDD")
		val rddFromFile = spark.sparkContext.textFile("s3a://sparkbyexamples/csv/text01.txt")
		println(rddFromFile.getClass)
		
		println("##Get data Using collect")
		rddFromFile.collect().foreach(f=>{
			println(f)
		})

30. How to change column types in Spark SQL DataFrame?
		val df2 = df.withColumn("yearTmp", df.year.cast(IntegerType))
			.drop("year")
			.withColumnRenamed("yearTmp", "year")
		
31. I've got big RDD(1gb) in yarn cluster. I can't use collect() How to handle this?
		RDD.toLocalIterator method an efficient way to do the job. It uses runJob to evaluate only a single partition on each step.
		As for the toLocalIterator, it is used to collect the data from the RDD scattered around your cluster into one only node, the one from which the program is running, and do something with all the data in the same node. It is similar to the collect method, but instead of returning a List it will return an Iterator.
		rdd.toLocalIterator()

32. Is there any way for Spark to create primary keys?
		df.withColumn("id", monotonicallyIncreasingId).show()
		monotonicallyIncreasingId function which should work just fine as long as you don't require consecutive numbers.
		
33. How to add a constant column in a Spark DataFrame?
		val newdf = df.withColumn("newcol",lit("myval"))

34. While processing CSV file resultant output is multiple file, wanted single file?

35. How do I skip a header from CSV files in Spark?

36. Explain the difference between Spark SQL and Hive?

37. How to transpose Spark DataFrame?
		val new_schema = StructType(df1.select(collect_list("Column")).first().getAs[Seq[String]](0).map(z => StructField(z, StringType)))
		val new_values = sc.parallelize(Seq(Row.fromSeq(df.select(collect_list("Value")).first().getAs[Seq[String]](0))))
		sqlContext.createDataFrame(new_values, new_schema).show(false)
		
38. What is narrow and wide dependency?
39. Difference between Aggregatebykey & combinebykey?
40. How to disable broadcast?
41. How to create replication in spark?
42. Draw Spark architecture.
43. How to add one row in dataframe?
44. How to add column in dataframe?
45. what is exploid function in spark?
46. What is Speculation execution?
47. Create dataframe with JSON file and display some selected fields.
48. How to Search table in hive through Spark?
49. Aggregate function in spark and how its working?
50. How to achive increamental insert and upload in Spark?
51. How many job and Stage will be there in spark?
52. How to assign flag D for duplicates and v for unique based on name field in dataframe?
53. What is partitioner in spark?
54. Why are we using AVRO fileformat in ingestion of data instead of parquet file?
55. How to check some partitioned are skewed in spark?
56. Accumulator is accessible from node or not?
57. 
