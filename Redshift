Redshift
###################
It is columnar datawarehouse used for OLAP systems.

--> Architecture
	1 leader node, multiple compute nodes
	Leader node
		has the SQL endpoint
		coordinates parallel query execution
	Compute node
		queries are executed in parallel in multiple nodes
		scale out / up/ down
		10 GBs network
	Runs in a single AZ (high-speed network communication)
	Instance types: dc2 (dense compute), and dense storage (ds2)
	Data stored on multiple nodes, dedicated CPU, memory and local storage
--> Redshift integrations in AWS:
	COPY command
		to load data from:S3,DynamoDB.
		includes reference to IAM role.
		
--> Table Design
	Distribution styles
		Even (default): rows distributed accross slices regardless of values in a particular column. 
		Key: Used to optimize JOINs. same key should be in the same slice to be faster. Values are hashed, same value goes to same location.
		All: Full table data goes to the first slice of every node
		Auto: Combines EVEN and ALL.
		
	Sort Keys
		Similar to indexes. They are not indexes.
		Redshift uses block size of 1MB. Less IO seeks
		Sort order allows quicker reading.
		compound (default): 
			Includes all columns stated. Order matters (sames as mysql indexes). 
			Must include prefix of columns (same order).
			vacuum and analyze operations used for improving performance.
		interleaved: 
			all columns are equally important.
			Data load and vacuum is slower
			Use on very large tables
			Not good for loading data in sort order
			
	Compression:
		ENCODE lzo, mostly32, etc. reduce size
		Redshift support automatic compression. Redshift automatically figures out a good encoding by running ANALYZE command behind the scenes on table creation (only first time).
		To compress mannually we need run ANALIZE compression table_name, which suggests compression.
		
	Constraint
		column-level:
			PRIMARY KEY
			UNIQUE (it's not enforceable by redshift)
			NOT NULL / NULL (the only that is enforced)
			REFERENCES (same as FK), not enforceable by redshift

--> Loading data to redshift:
	Don't use single inserts, use COPY command:
		from S3, dynamoDB, EMR, EC2
		split files (use same prefix), and copy will load in parallel, faster.
		
	Services that need to go through S3:
		Kinesis firehose
		kinesis app
		AWS DB Migration Service
		
	Can use lambda to run the copy command triggered on S3.
	Splitting files
		Use number of files = number of slices or a multiple e.g. 4 slices -> 4, 8, 12, .. files
		Keep file sizes even.
		
	Support file format:
		CSV, delimited, JSON, fixed width, Avro, ORC, Parquet, Text
		
	When copy command fails, check tables STL_LOAD_ERRORS, STL_LOADERROR_DETAIL
	
	Redshift does not support UPSERT.
	
--> Unloading data (LOAD command)
		To write selected data to file from table.

Hands-on
##############################
--> Sort: Can be applied on one or multiple columns.
		CREATE TABLE deep_dive (
		aid INT
		,loc CHAR(3)
		,dt DATE
		) SORTKEY (dt, loc);

--> Distribution:
		CREATE TABLE deep_dive (
		aid INT --audience_id
		,loc CHAR(3) --location
		,dt DATE --date
		) (EVEN|KEY|ALL|AUTO);
		
		
		CREATE TABLE deep_dive (
		aid INT --audience_id
		,loc CHAR(3) --location
		,dt DATE
		) DISTSTYLE EVEN;
		
		
		CREATE TABLE deep_dive (
		aid INT --audience_id
		,loc CHAR(3) --locatio
		,dt DATE --date
		) DISTSTYLE KEY DISTKEY (loc);
		
		
		CREATE TABLE deep_dive (
		aid INT --audience_id
		,loc CHAR(3) --location
		,dt DATE
		) DISTSTYLE ALL;
		
--> Unload:
			UNLOAD ('Select * from deep_dive')
			TO 's3://bucket/dd.csv'
			CREDENTIALS ' ARN '
			DELIMITER ','
			PARALLEL OFF
			
